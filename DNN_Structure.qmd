---
title: "Deep Neural Network Framework"
author: "David Jolly"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---
# Set up Global Policies and source in custom funcitons. 
```{r}
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
```

## Load required packages
```{r}
library(tidyverse)
library(tidymodels) 
library(xfun, include.only = "cache_rds") 
library(keras, exclude = "get_weights")
library(magrittr, exclude = c("set_names", "extract"))
library(yardstick)
```

## Load custom plotting and EDA functions
```{r}
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
```

## Set the them and include a data path
```{r}
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)
rerun_setting <- FALSE
path_data <- "Fetch"
```

## Include parallel processing for future work.
```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```
I enabled parallel processing to speed up model tuning and training.

# Read in data 
```{r}
data_all <- read_csv(here::here(path_data, 'fictional_data_shopping.csv'), col_types = cols()) |>
  janitor::clean_names() |> 
  glimpse()
```
I read in the dataset and used clean_names() to convert all variable names to snake_case, then previewed the structure of the data.

## Remove shopping_summary, it will not be included in my analysis
```{r}
data_all <- data_all |> 
  select(-shopping_summary, -id)
```
I removed shopping_summary and id because theyâ€™re not needed for modeling.



# Exploratory Data Analysis
```{r}
data_all <- data_all |> 
  mutate(customer_type = factor(customer_type, levels = c("New", "Returning"),
                                labels = c("New", "Returning")),
         shopping_day = factor(shopping_day, level = c("Weekday", "Weekend"),
                               labels = c("Weekday", "Weekend")),
         store = factor(store),
         payment_method = factor(payment_method, levels = c("Gift Card", "Credit Card", "Mobile Payment", "Cash")))


data_all <- data_all |> 
  filter(spend_per_item >= 0)
```
I converted categorical columns to factors and removed any rows where the target variable spend_per_item is invalid 

# Split the data
```{r}
set.seed(123456)
splits_test <- data_all |> 
  initial_split(prop = 2/3, strata = "spend_per_item")

data_trn <- splits_test |> 
  analysis()

data_test <- splits_test |> 
  assessment()

split_val <- validation_split(data_trn, prop = c(3/4), strata = "spend_per_item")
```
I split the dataset into training (2/3) and testing (1/3) sets, stratifying on the outcome. I also created a validation split to be used internally for tuning.

## Visualize new data
```{r}
data_trn <- data_trn |> 
  filter(spend_per_item >= 0)

data_trn |>
  select(where(is.numeric)) |> 
  names() |> 
  map(\(name) plot_box_violin(df = data_trn, x = name)) |> 
  cowplot::plot_grid(plotlist = _, ncol = 3)
```
It looks like we have some positively skewed data for all but time_spent_min. 


# DNN
## Create a recipe for spend_per_item as the outcome variable
```{r}
rec_spend <- recipe(spend_per_item ~ ., data = data_trn) |>
  step_unknown(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors()) |>
  step_interact(terms = ~ amount_spent:shopping_day_Weekend) |> 
  step_YeoJohnson(all_numeric_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

feat_nrom_trn <- rec_spend |> 
  prep(data_trn) |> 
  bake(NULL)
```
I built a recipe that includes dummy coding, an interaction term, skewness transformation (Yeo-Johnson), and normalization. This prepares data for neural network modeling.


## Create a function to optimize parameters
```{r}
deep_model_001 <- function(hidden_units = 60, dropout = 0.1, activation = "relu", ...) {
  function(x, y = NULL) {
    keras_model_sequential() |>
      layer_dense(units = hidden_units * 2, activation = activation, input_shape = ncol(x)) |>
      layer_dropout(rate = dropout) |>
      layer_dense(units = hidden_units, activation = activation) |>
      layer_dropout(rate = dropout) |>
      layer_dense(units = hidden_units, activation = activation) |>
      layer_dense(units = 1) |>
      compile(
        loss = "mse",
        optimizer = optimizer_adam(learning_rate = 0.001),  
        metrics = list("mean_absolute_error")
      )
  }
}

early_stop_cb <- callback_early_stopping(
  monitor = "val_loss",
  patience = 15,
  restore_best_weights = TRUE,
  min_delta = 0.0001
)
```
I defined a flexible neural network architecture using keras_model_sequential(). It supports tuning key hyperparameters like hidden units, dropout, and activation.


```{r}

config_grid <- expand_grid(
  hidden_units = c(20, 40),
  dropout = c(0.1, 0.2),
  activation = c("relu", "selu")
)


lr_scheduler <- callback_reduce_lr_on_plateau(
  monitor = "val_loss",
  factor = 0.5,
  patience = 5,
  min_lr = 0.00001
)
```
These callbacks stop training early if the model stops improving and reduce the learning rate when validation loss plateaus. I created a grid of hyperparameter combinations to search over during tuning.

```{r}
set.seed(42)
splits_kfold <- vfold_cv(data_trn, v = 5, strata = spend_per_item)
```
I used 5-fold cross-validation with stratification to evaluate model performance.

# Fit model
```{r}
set.seed(102030)
fit_seeds <- sample.int(10^5, size = 3)

fits_spend <- tune_grid(
 mlp(epochs = 100,
    hidden_units = tune(),
    dropout = tune(),
    activation = tune()) |>
  set_mode("regression") |>
  set_engine("keras",
             verbose = 0,
           callbacks = list(early_stop_cb, lr_scheduler),
           seeds = fit_seeds,
           custom_model = deep_model_001),
  preprocessor = rec_spend,
  grid = config_grid,
  resamples = splits_kfold,
  metrics = metric_set(rmse, mae),
  control = control_grid(verbose = TRUE)
)

show_best(fits_spend)
select_best(fits_spend)

```
I used tune_grid() to train multiple models across different configurations and folds. This helps identify the best-performing neural network architecture. Then used show_best() and select_best() to get my best model to give me my highest performing model.

## Bake on Test Set
```{r}
feat_trn_best <- rec_spend |> 
  prep(data_trn) |> 
  bake(NULL)

feat_test_best <- rec_spend |> 
  prep(data_trn) |> 
  bake(data_test)
```
I want to see how this data performs in the test set, so I bake the held-out data. 

# Best Model
```{r}
best_params <- fits_spend |> 
  select_best(metric = "rmse")


best_model <- mlp(epochs = 100,
                  hidden_units = best_params$hidden_units,
                  dropout = best_params$dropout,
                  activation = best_params$activation) |> 
  set_mode("regression") |> 
  set_engine("keras", 
             verbose = 0, 
             callbacks = list(early_stop_cb, lr_scheduler),
             seeds = fit_seeds,
             custom_model = deep_model_001) |> 
  fit(spend_per_item ~ ., data = feat_trn_best)

```
I selected the best hyperparameter combination and retrained the model on the full training set.

# Make and save predicitions
```{r}
feat_test_best |> 
  mutate(spend_per_item = predict(best_model, feat_test_best)$.pred) |> 
  select(spend_per_item) |> 
  glimpse() |> 
  write_csv(here::here(path_data, "test_preds_TA.csv"))
```
I used the final model to make predictions on the test set and saved the output as a CSV.



```{r}
rmse_vec(
  truth = data_test$spend_per_item,
  estimate = predict(best_model, new_data = feat_test_best)$.pred
)

rmse_vec(
  truth = data_trn$spend_per_item,
  estimate = predict(best_model, new_data = feat_nrom_trn)$.pred
)
```
I evaluated model performance using RMSE on both training and test data to check for overfitting and overall accuracy.


# Summary
I plan on using this as a resource on how to create a DNN. My data is too small for a Deep Neural Network with these many hidden layers. When I reference this document, it is important to account for the type and size of my data. This project overfit the training set, but produced a low RMSE regardless.
 









